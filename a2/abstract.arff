% Man Tik Li
% abstract.arff
% INFO 371 - Data Mining Applications
% Domain-class:
% AI - Artificial Intelligence (Line 15 - 34)
% STAT - Statistics (Line 35 - 54)
% ENVS - Environment Science (Line 55 - 74)

@RELATION hw2-articles

@ATTRIBUTE abstract_col string
@ATTRIBUTE domain-class {AI, STAT, ENVS}

@Data
"IBM on Thursday acknowledged the challenge and embraced the opportunity for the company in the accelerating shift to cloud computing. The company said it was spinning off its legacy technology services business to focus on cloud computing and artificial intelligence. Arvind Krishna, who became chief executive this year, called the move “a landmark day” for IBM, “redefining the company.” The split-up strategy reflects how decisively computing has shifted to the cloud. Today, nearly all news software is being created as a cloud service, delivered over the internet from remote data centers. The computing model affords corporate customers more flexibility and cost savings, sold as a pay-for-use service or annual subscriptions. IBM was late to the cloud market, which Amazon pioneered when it began Amazon Web Services in 2006. But IBM has made a major push into cloud services and software in recent years, punctuated by its $34 billion purchase in 2018 of Red Hat, a distributor of open-source software and tools used by cloud developers. In an interview, Ginni Rometty, IBM’s executive chair and former chief executive, said cloud computing, enhanced by artificial intelligence, “is now IBM’s enduring platform.”",AI
"Of course, there were calculating devices before the electronic computer. The earliest automated machines, dating from the 17th century, were discussed on page 6. The first pro- grammable machine was a loom, devised in 1805 by Joseph Marie Jacquard (1752–1834), that used punched cards to store instructions for the pattern to be woven. In the mid-19th century, Charles Babbage (1792–1871) designed two machines, neither of which he com- pleted. The Difference Engine was intended to compute mathematical tables for engineering and scientific projects. It was finally built and shown to work in 1991 at the Science Museum in London (Swade, 2000). Babbage’s Analytical Engine was far more ambitious: it included addressable memory, stored programs, and conditional jumps and was the first artifact capa- ble of universal computation. Babbage’s colleague Ada Lovelace, daughter of the poet Lord Byron, was perhaps the world’s first programmer. (The programming language Ada is named after her.) She wrote programs for the unfinished Analytical Engine and even speculated that the machine could play chess or compose music. AI also owes a debt to the software side of computer science, which has supplied the operating systems, programming languages, and tools needed to write modern programs (and papers about them). But this is one area where the debt has been repaid: work in AI has pio- neered many ideas that have made their way back to mainstream computer science, including time sharing, interactive interpreters, personal computers with windows and mice, rapid development environments, the linked list data type, automatic storage management, and key concepts of symbolic, functional, declarative, and object-oriented programming.",AI
"Modern control theory, especially the branch known as stochastic optimal control, has as its goal the design of systems that maximize an objective function over time. This roughly matches our view of AI: designing systems that behave optimally. Why, then, are AI and control theory two different fields, despite the close connections among their founders? The answer lies in the close coupling between the mathematical techniques that were familiar to the participants and the corresponding sets of problems that were encompassed in each world view. Calculus and matrix algebra, the tools of control theory, lend themselves to systems that are describable by fixed sets of continuous variables, whereas AI was founded in part as a way to escape from the these perceived limitations. The tools of logical inference and computation allowed AI researchers to consider problems such as language, vision, and planning that fell completely outside the control theorist’s purview.",AI
"The first work that is now generally recognized as AI was done by Warren McCulloch and Walter Pitts (1943). They drew on three sources: knowledge of the basic physiology and function of neurons in the brain; a formal analysis of propositional logic due to Russell and Whitehead; and Turing’s theory of computation. They proposed a model of artificial neurons in which each neuron is characterized as being “on” or “off,” with a switch to “on” occurring in response to stimulation by a sufficient number of neighboring neurons. The state of a neuron was conceived of as “factually equivalent to a proposition which proposed its adequate stimulus.” They showed, for example, that any computable function could be computed by some network of connected neurons, and that all the logical connectives (and, or, not, etc.) could be implemented by simple net structures. McCulloch and Pitts also suggested that suitably defined networks could learn. Donald Hebb (1949) demonstrated a simple updating rule for modifying the connection strengths between neurons. His rule, now called Hebbian learning, remains an influential model to this day.",AI
"There were a number of early examples of work that can be characterized as AI, but Alan Turing’s vision was perhaps the most influential. He gave lectures on the topic as early as 1947 at the London Mathematical Society and articulated a persuasive agenda in his 1950 article “Computing Machinery and Intelligence.” Therein, he introduced the Turing Test, machine learning, genetic algorithms, and reinforcement learning. He proposed the Child Programme idea, explaining “Instead of trying to produce a programme to simulate the adult mind, why not rather try to produce one which simulated the child’s?” Princeton was home to another influential figure in AI, John McCarthy. After receiving his PhD there in 1951 and working for two years as an instructor, McCarthy moved to Stan- ford and then to Dartmouth College, which was to become the official birthplace of the field. McCarthy convinced Minsky, Claude Shannon, and Nathaniel Rochester to help him bring together U.S. researchers interested in automata theory, neural nets, and the study of intel- ligence. They organized a two-month workshop at Dartmouth in the summer of 1956. The proposal",AI
"The Dartmouth workshop did not lead to any new breakthroughs, but it did introduce all the major figures to each other. For the next 20 years, the field would be dominated by these people and their students and colleagues at MIT, CMU, Stanford, and IBM. Looking at the proposal for the Dartmouth workshop (McCarthy et al., 1955), we can see why it was necessary for AI to become a separate field. Why couldn’t all the work done in AI have taken place under the name of control theory or operations research or decision theory, which, after all, have objectives similar to those of AI? Or why isn’t AI a branch of mathematics? The first answer is that AI from the start embraced the idea of duplicating human faculties such as creativity, self-improvement, and language use. None of the other fields were addressing these issues. The second answer is methodology. AI is the only one of these fields that is clearly a branch of computer science (although operations research does share an emphasis on computer simulations), and AI is the only field to attempt to build machines that will function autonomously in complex, changing environments.",AI
"The early years of AI were full of successes—in a limited way. Given the primitive comput- ers and programming tools of the time and the fact that only a few years earlier computers were seen as things that could do arithmetic and no more, it was astonishing whenever a com- puter did anything remotely clever. The intellectual establishment, by and large, preferred to believe that “a machine can never do X.” (See Chapter 26 for a long list of X’s gathered by Turing.) AI researchers naturally responded by demonstrating one X after another. John McCarthy referred to this period as the “Look, Ma, no hands!” era. Newell and Simon’s early success was followed up with the General Problem Solver, or GPS. Unlike Logic Theorist, this program was designed from the start to imitate human problem-solving protocols. Within the limited class of puzzles it could handle, it turned out that the order in which the program considered subgoals and possible actions was similar to that in which humans approached the same problems. Thus, GPS was probably the first pro- gram to embody the “thinking humanly” approach. The success of GPS and subsequent pro- grams as models of cognition led Newell and Simon (1976) to formulate the famous physical symbol system hypothesis, which states that “a physical symbol system has the necessary and sufficient means for general intelligent action.” What they meant is that any system (human or machine) exhibiting intelligence must operate by manipulating data structures composed of symbols. We will see later that this hypothesis has been challenged from many directions.",AI
"At IBM, Nathaniel Rochester and his colleagues produced some of the first AI pro- grams. Herbert Gelernter (1959) constructed the Geometry Theorem Prover, which was able to prove theorems that many students of mathematics would find quite tricky. Starting in 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventually learned to play at a strong amateur level. Along the way, he disproved the idea that computers can do only what they are told to: his program quickly learned to play a better game than its creator. The program was demonstrated on television in February 1956, creating a strong impression. Like Turing, Samuel had trouble finding computer time. Working at night, he used machines that were still on the testing floor at IBM’s manufacturing plant. Chapter 5 covers game playing, and Chapter 21 explains the learning techniques used by Samuel. John McCarthy moved from Dartmouth to MIT and there made three crucial contribu- tions in one historic year: 1958. In MIT AI Lab Memo No. 1, McCarthy defined the high-level language Lisp, which was to become the dominant AI programming language for the next 30 years. With Lisp, McCarthy had the tool he needed, but access to scarce and expensive com- puting resources was also a serious problem. In response, he and others at MIT invented time sharing.",AI
"1958 also marked the year that Marvin Minsky moved to MIT. His initial collaboration with McCarthy did not last, however. McCarthy stressed representation and reasoning in for- mal logic, whereas Minsky was more interested in getting programs to work and eventually developed an anti-logic outlook. In 1963, McCarthy started the AI lab at Stanford. His plan to use logic to build the ultimate Advice Taker was advanced by J. A. Robinson’s discov- ery in 1965 of the resolution method (a complete theorem-proving algorithm for first-order logic; see Chapter 9). Work at Stanford emphasized general-purpose methods for logical reasoning. Applications of logic included Cordell Green’s question-answering and planning systems (Green, 1969b) and the Shakey robotics project at the Stanford Research Institute (SRI). The latter project, discussed further in Chapter 25, was the first to demonstrate the complete integration of logical reasoning and physical activity. Minsky supervised a series of students who chose limited problems that appeared to require intelligence to solve. These limited domains became known as microworlds. James Slagle’s SAINT program (1963) was able to solve closed-form calculus integration problems typical of first-year college courses. Tom Evans’s ANALOGY program (1968) solved geo- metric analogy problems that appear in IQ tests. Terms such as “visible future” can be interpreted in various ways, but Simon also made more concrete predictions: that within 10 years a computer would be chess champion, and a significant mathematical theorem would be proved by machine. These predictions came true (or approximately true) within 40 years rather than 10. Simon’s overconfidence was due to the promising performance of early AI systems on simple examples. In almost all cases, however, these early systems turned out to fail miserably when tried out on wider selections of problems and on more difficult problems. The first kind of difficulty arose because most early programs knew nothing of their subject matter; they succeeded by means of simple syntactic manipulations. A typical story occurred in early machine translation efforts, which were generously funded by the U.S. Na- tional Research Council in an attempt to speed up the translation of Russian scientific papers in the wake of the Sputnik launch in 1957.",AI
"The second kind of difficulty was the intractability of many of the problems that AI was attempting to solve. Most of the early AI programs solved problems by trying out different combinations of steps until the solution was found. This strategy worked initially because microworlds contained very few objects and hence very few possible actions and very short solution sequences. Before the theory of computational complexity was developed, it was widely thought that “scaling up” to larger problems was simply a matter of faster hardware and larger memories. The optimism that accompanied the development of resolution theorem proving, for example, was soon dampened when researchers failed to prove theorems involv- ing more than a few dozen facts. The fact that a program can find a solution in principle does not mean that the program contains any of the mechanisms needed to find it in practice. The illusion of unlimited computational power was not confined to problem-solving programs. Early experiments in machine evolution (now called genetic algorithms) (Fried- berg, 1958; Friedberg et al., 1959) were based on the undoubtedly correct belief that by making an appropriate series of small mutations to a machine-code program, one can gen- erate a program with good performance for any particular task. The idea, then, was to try random mutations with a selection process to preserve mutations that seemed useful. De- spite thousands of hours of CPU time, almost no progress was demonstrated. Modern genetic algorithms use better representations and have shown more success.",AI
"Failure to come to grips with the “combinatorial explosion” was one of the main criti- cisms of AI contained in the Lighthill report (Lighthill, 1973), which formed the basis for the decision by the British government to end support for AI research in all but two universities. (Oral tradition paints a somewhat different and more colorful picture, with political ambitions and personal animosities whose description is beside the point.) A third difficulty arose because of some fundamental limitations on the basic structures being used to generate intelligent behavior. For example, Minsky and Papert’s book Percep- trons (1969) proved that, although perceptrons (a simple form of neural network) could be shown to learn anything they were capable of representing, they could represent very little. In particular, a two-input perceptron (restricted to be simpler than the form Rosenblatt originally studied) could not be trained to recognize when its two inputs were different. Although their results did not apply to more complex, multilayer networks, research funding for neural-net research soon dwindled to almost nothing. Ironically, the new back-propagation learning al- gorithms for multilayer networks that were to cause an enormous resurgence in neural-net research in the late 1980s were actually discovered first in 1969 (Bryson and Ho, 1969).",AI
"The picture of problem solving that had arisen during the first decade of AI research was of a general-purpose search mechanism trying to string together elementary reasoning steps to find complete solutions. Such approaches have been called weak methods because, although general, they do not scale up to large or difficult problem instances. The alternative to weak methods is to use more powerful, domain-specific knowledge that allows larger reasoning steps and can more easily handle typically occurring cases in narrow areas of expertise. One might say that to solve a hard problem, you have to almost know the answer already. The DENDRAL program (Buchanan et al., 1969) was an early example of this approach. It was developed at Stanford, where Ed Feigenbaum (a former student of Herbert Simon), Bruce Buchanan (a philosopher turned computer scientist), and Joshua Lederberg (a Nobel laureate geneticist) teamed up to solve the problem of inferring molecular structure from the information provided by a mass spectrometer. The input to the program consists of the ele- mentary formula of the molecule (e.g., C6H13NO2) and the mass spectrum giving the masses of the various fragments of the molecule generated when it is bombarded by an electron beam. For example, the mass spectrum might contain a peak at m = 15, corresponding to the mass of a methyl (CH3) fragment.",AI
"Species (1997) suggests that this is the defining characteristic of humans—but the most ar- dent connectionists questioned whether symbol manipulation had any real explanatory role in detailed models of cognition. This question remains unanswered, but the current view is that connectionist and symbolic approaches are complementary, not competing. As occurred with the separation of AI and cognitive science, modern neural network research has bifurcated into two fields, one concerned with creating effective network architectures and algorithms and understanding their mathematical properties, the other concerned with careful modeling of the empirical properties of actual neurons and ensembles of neurons. In the early period of AI it seemed plausible that new forms of symbolic computation, e.g., frames and semantic networks, made much of classical theory obsolete. This led to a form of isolationism in which AI became largely separated from the rest of computer science. This isolationism is currently being abandoned. There is a recognition that machine learning should not be isolated from information theory, that uncertain reasoning should not be isolated from stochastic modeling, that search should not be isolated from classical optimization and control, and that automated reasoning should not be isolated from formal methods and static analysis.",AI
"Judea Pearl’s (1988) Probabilistic Reasoning in Intelligent Systems led to a new accep- tance of probability and decision theory in AI, following a resurgence of interest epitomized by Peter Cheeseman’s (1985) article “In Defense of Probability.” The Bayesian network formalism was invented to allow efficient representation of, and rigorous reasoning with, uncertain knowledge. This approach largely overcomes many problems of the probabilistic reasoning systems of the 1960s and 1970s; it now dominates AI research on uncertain reason- ing and expert systems. The approach allows for learning from experience, and it combines the best of classical AI and neural nets. Work by Judea Pearl (1982a) and by Eric Horvitz and David Heckerman (Horvitz and Heckerman, 1986; Horvitz et al., 1986) promoted the idea of normative expert systems: ones that act rationally according to the laws of decision theory and do not try to imitate the thought steps of human experts. The WindowsTM operating sys- tem includes several normative diagnostic expert systems for correcting problems. Chapters 13 to 16 cover this area. Similar gentle revolutions have occurred in robotics, computer vision, and knowledge representation. A better understanding of the problems and their complexity properties, com- bined with increased mathematical sophistication, has led to workable research agendas and robust methods. Although increased formalization and specialization led fields such as vision and robotics to become somewhat isolated from “mainstream” AI in the 1990s, this trend has reversed in recent years as tools from machine learning in particular have proved effective for many problems. The process of reintegration is already yielding significant benefits",AI
"One consequence of trying to build complete agents is the realization that the previously isolated subfields of AI might need to be reorganized somewhat when their results are to be tied together. In particular, it is now widely appreciated that sensory systems (vision, sonar, speech recognition, etc.) cannot deliver perfectly reliable information about the environment. Hence, reasoning and planning systems must be able to handle uncertainty. A second major consequence of the agent perspective is that AI has been drawn into much closer contact with other fields, such as control theory and economics, that also deal with agents. Recent progress in the control of robotic cars has derived from a mixture of approaches ranging from better sensors, control-theoretic integration of sensing, localization and mapping, as well as a degree of high-level planning. Despite these successes, some influential founders of AI, including John McCarthy (2007), Marvin Minsky (2007), Nils Nilsson (1995, 2005) and Patrick Winston (Beal and Winston, 2009), have expressed discontent with the progress of AI. They think that AI should put less emphasis on creating ever-improved versions of applications that are good at a spe- cific task, such as driving a car, playing chess, or recognizing speech. Instead, they believe AI should return to its roots of striving for, in Simon’s words, “machines that think, that learn and that create.” They call the effort human-level AI or HLAI; their first symposium was in 2004 (Minsky et al., 2004). The effort will require very large knowledge bases; Hendler et al. (1995) discuss where these knowledge bases might come from.",AI
"Throughout the 60-year history of computer science, the emphasis has been on the algorithm as the main subject of study. But some recent work in AI suggests that for many problems, it makes more sense to worry about the data and be less picky about what algorithm to apply. This is true because of the increasing availability of very large data sources: for example, trillions of words of English and billions of images from the Web (Kilgarriff and Grefenstette, 2006); or billions of base pairs of genomic sequences (Collins et al., 2003). One influential paper in this line was Yarowsky’s (1995) work on word-sense disam- biguation: given the use of the word “plant” in a sentence, does that refer to flora or factory? Previous approaches to the problem had relied on human-labeled examples combined with machine learning algorithms. Yarowsky showed that the task can be done, with accuracy above 96%, with no labeled examples at all.",AI
"As another example, Hays and Efros (2007) discuss the problem of filling in holes in a photograph. Suppose you use Photoshop to mask out an ex-friend from a group photo, but now you need to fill in the masked area with something that matches the background. Hays and Efros defined an algorithm that searches through a collection of photos to find something that will match. They found the performance of their algorithm was poor when they used a collection of only ten thousand photos, but crossed a threshold into excellent performance when they grew the collection to two million photos. Work like this suggests that the “knowledge bottleneck” in AI—the problem of how to express all the knowledge that a system needs—may be solved in many applications by learn- ing methods rather than hand-coded knowledge engineering, provided the learning algorithms have enough data to go on (Halevy et al., 2009). Reporters have noticed the surge of new ap- plications and have written that “AI Winter” may be yielding to a new Spring (Havenstein, 2005). As Kurzweil (2005) writes, “today, many thousands of AI applications are deeply embedded in the infrastructure of every industry.”",AI
"Robotic vehicles: A driverless robotic car named STANLEY sped through the rough terrain of the Mojave dessert at 22 mph, finishing the 132-mile course first to win the 2005 DARPA Grand Challenge. STANLEY is a Volkswagen Touareg outfitted with cameras, radar, and laser rangefinders to sense the environment and onboard software to command the steer- ing, braking, and acceleration (Thrun, 2006). The following year CMU’s BOSS won the Ur- ban Challenge, safely driving in traffic through the streets of a closed Air Force base, obeying traffic rules and avoiding pedestrians and other vehicles. Speech recognition: A traveler calling United Airlines to book a flight can have the en- tire conversation guided by an automated speech recognition and dialog management system. Autonomous planning and scheduling: A hundred million miles from Earth, NASA’s Remote Agent program became the first on-board autonomous planning program to control the scheduling of operations for a spacecraft (Jonsson et al., 2000). REMOTE AGENT gen- erated plans from high-level goals specified from the ground and monitored the execution of those plans—detecting, diagnosing, and recovering from problems as they occurred. Succes- sor program MAPGEN (Al-Chang et al., 2004) plans the daily operations for NASA’s Mars Exploration Rovers, and MEXAR2 (Cesta et al., 2007) did mission planning—both logistics and science planning—for the European Space Agency’s Mars Express mission in 2008.",AI
"A key feature of our world is its uncertainty. Will it rain today? If we leave potato salad outdoors in the summer, what is the chance it will make us sick? If we collect a group of bears and measure their respiratory rates, how certain are we of their average respiratory rate? Atmospheric CO2 levels appear to be increasing; how certain are we that these changes represent a real trend? This is the stuff of statistics. Scientists must know how to sam- ple, organize, and describe the data, and say to what degree any answer reflects the larger popula- tion of interest. Statistics allows us to quantify the level of uncertainty about what we know and to make broader generalizations. According to Snedecor and Cochran (1989), sta- tistics includes the techniques for “collecting, ana- lyzing, and drawing conclusions from data.” Statistics, in some ways, is the study of uncertainty. Its goals include assigning probabilities to the reli- ability of estimates and conclusions, and to the like- lihood of future events. Statistics, at its best, is a formal way of thinking about the physical universe.",AI
"(McCarthy et al., 1955) made the assertion that “Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.” Thus, AI was founded on the assumption that weak AI is possible. Others have asserted that weak AI is impossible: “Artificial intelligence pursued within the cult of computationalism stands not even a ghost of a chance of producing durable results” (Sayre, 1993). Clearly, whether AI is impossible depends on how it is defined. In Section 1.1, we de- fined AI as the quest for the best agent program on a given architecture. With this formulation, AI is by definition possible: for any digital architecture with k bits of program storage there are exactly 2k agent programs, and all we have to do to find the best one is enumerate and test them all. This might not be feasible for large k, but philosophers deal with the theoretical, not the practical.",AI
"Often the reason for such media fallacies lies in the source of information, like preliminary results presented at a scientific meeting, which the reporter may report with more certainty than the results should merit. A catchy headline in one media outlet can quickly be repeated by others and the number of headlines makes a story seem more real than it should be (the trending phenomenon). In some cases, one study with a differing set of results is pit- ted equally against a mountain of evidence from many other studies to provide a counterpoint (con- flict is inherently more appealing than agreement). The best media reports include both the source for their information, as well as some numerical infor- mation to provide appropriate context. A firm un- derstanding of statistics provides the reader with a healthy bit of skepticism, allowing one to sort out the weak from the strong conclusions. Readers of scientific literature regularly see sta- tistics. Pick up any professional journal, whether it focuses on molecular biology or ecology, and you are bound to run across statistics. In all of these fields, statistics is used by scientists to organize and describe data and to help evaluate the results from experiments and observational surveys. Physiolo- gists use regression models to explore the response of metabolic rates on temperature. Multivariate procedures are used with gene sequence data to ex- plore the evolutionary relationships within taxo- nomic groups such as apes, flowering plants, and influenza viruses. Statistics is an important tool for public health, helping",STAT
"Statistics is also widely used in product safety, through guiding the effi- cient sampling of food for pathogenic microbes or in setting appropriate standards of quality. Practicing scientists use statistical procedures at all stages of their research. One use of statistics is to provide a summary of quantitative data. Graphs and tables are employed in most of the chapters in this book. Descriptive statistics reduce a mass of numbers to something more comprehensible, illus- trating both the average and variation (chapter 4). A very important use of statistics is statistical infer- ence: making conclusions about properties of a broader population based on a sample from that population. Inference includes estimation, in which we determine with a specified degree of uncer- tainty some population characteristic (chapter 9). Inference also includes hypothesis testing (chapter 6), in which we use observable data to answer a particular question about a group of interest. Does the growth rate of white oak trees increase with ele- vated CO2 concentration in the air? Has the world- wide incidence of influenza gone down since the introduction of the flu vaccine? Only statistical hy- pothesis testing can provide the answer. Without it, we are just stating opinions. Baseball has descrip- tive statistics, but no hypothesis testing, leading to endless arguments about what the numbers mean.",STAT
"Many scientists now use statistical computer programs to generate the random numbers. At the end of this chapter (immediately after exercises), you will find Applications in R, where we illus- trate the same numerical example using R. This is a versatile computer package that is free and widely used. General directions for obtaining and using R are given in appendix C, in the back of the book. This general procedure for collecting a simple random sample can be applied to a variety of situa- tions. For example, we might want to sample from a list of patients in a clinic; simply give each patient an ID number (here N is the total number of pa- tients). We could do the same procedure for sam- pling different batches of food for bacteria checks or selecting particular hours during the week to collect air quality measurements in a city. By now, you may be saying that this random- ization procedure seems like a lot of work. How- ever, collecting truly random samples is generally not possible any other way. (Closing your eyes and pointing to locations on the map is not truly ran- dom; this process is haphazard.) After some prac- tice, this procedure gets easier. One useful bit of advice: create your random number list before going into the field (or the lab). This procedure is more easily done in your office than in the car or at the lab bench!",STAT
"Before we get too far with statistics, it’s impor- tant that we develop a common vocabulary about the sorts of things that we measure and count, as well as about how the data that are generated can be organized. In this chapter we will first examine the different sorts of variables encountered in statistics and how they are represented. We will then explore how numerical data are manipulated and inter- preted. Finally, we will describe best practices for data management that will enhance the reliability of your own work and help maintain data access for future users. These practices are an important foun- dation for all students and practitioners of science. Variables can be classified by the sort of infor- mation that is recorded (categories or measure- ments) and also by whether changes in one variable may cause changes in another variable. We discuss each of these situations in the sections below. Variables that include only groups and not measurements are called categorical variables. These are classes of objects into which we can place an individual. Categorical variables can be, in turn, either attributes or ranks. Attributes lack any par- ticular logical order and include such properties as gender (male or female), political affiliation, and whether or not an individual ate a particular food at a picnic. Attributes are also sometimes called nomi- nal variables (names). ",STAT
"Rescaling a graph (such as placing the original values on log scale) is equivalent to doing a log transformation on the original data. If you are not convinced, try doing that with the numbers in ex- ample 2.1; plot [H+] on log scale against log10[H+] on linear scale. Do you get a straight line? There are many biological examples of continu- ous measurements that require log transformation prior to statistical analysis. The sizes of organisms in natural populations often have odd distributions. The bulk of the population is composed of small in- dividuals, but also includes a few individuals that are much larger (and therefore more influential in the population). To adequately analyze such data, it is often necessary to do a log transformation. A good example is plant size, as shown in example 2.3 below. Mathematical transformations are frequently used during statistical analyses in order to correct data for a failure to meet an assumption necessary for the test. You will see more about that topic later in the book (sections 8.4 and 11.4).",STAT
"Often, variables are described in terms of how we think they interact with other variables (the stuff of hypothesis testing!). Response variables are those whose variation we suspect depends on some other variable, called an explanatory variable (sometimes called predictor variable). For instance, suppose a microbiologist is interested in testing whether bacte- rial growth rate is affected by choice of carbon source; after incubating the bacteria in two different media (glucose and sucrose), she compares cell counts. Here the response variable is cell count and the explanatory variable is the medium. As another example, using regression analysis (chapter 14), a pharmacologist could test the influence of drug dos- age on blood pressure. Here, blood pressure is the response variable and drug dosage the predictor variable. You may have previously heard them re- ferred to as dependent and independent variables, respectively. However, these latter terms have other meanings and so are best avoided for the usage here. In any data analysis for hypothesis tests, choos- ing the correct statistical procedure depends on knowing both the types of variables involved (table 2.1) and which variable type is explanatory and which is response. You will see many examples throughout the book and later review this proce- dure in chapter 15.",STAT
"Quite often, data are lost for some reason or an- other. A sample was dumped, the equipment didn’t work that day, or the animal died. What should ap- pear in the spreadsheet? Don’t type a zero, since missing information is different from a value of zero. Don’t leave the cell blank; a blank may con- fuse others into thinking that you made a transcrip- tion error. Furthermore, some kinds of software interpret blanks as zeros. It’s best to type in a miss- ing data indicator, such as NA (not available). Doc- umentation for computer statistical programs generally describe what they expect to see for miss- ing data; for instance, R uses NA as its standard coding, while SAS uses a “.” and Minitab “*” for missing data indicators. In any case, the code for missing data should always be included as part of the metadata (section 2.6.1). Values that are extremely different from the ma- jority of values are called outliers. Some outliers are real, simply much larger or much smaller in value than is typical in nature (e.g., some people really are eight feet tall). But some outliers are due to error and so it’s very important to spot outliers early.",STAT
"Record clear metadata at an early stage of your investigation and keep this information in at least two places (e.g., a data notebook and typed into a place in an Excel file that is stored on a laptop and copied to “The Cloud”). You can easily add a text box to your spreadsheet where you place all the pertinent information, such as units of measure for each variable and explanations of variable names and any missing values. You can also note here if the data were checked (when and by whom). This level of documentation may seem like overkill, but it is always better to document more than you will ever need than it is to wonder what you were think- ing six weeks or six years ago. Start analyzing data right away. At the very least, do some plots and descriptive statistics that al- low you to see trends in the data. You may detect er- rors in procedures that can be corrected before you invest too much time in a project that isn’t going anywhere. Many beginning scientists have wasted months of work because of a silly mistake that could have been fixed early had they looked at their data.",STAT
"Large quantities of data are generally difficult to interpret by themselves without some method of assistance. Since humans are inherently visual or- ganisms, it makes sense that we rely on visual assis- tance for understanding and interpreting data. In this chapter and in the next on descriptive statistics, we consider some basic techniques for organizing and presenting data to make them more readily un- derstandable. Tables and graphs are widely used for data exploration by investigators and also to display summary data in published literature. In this chapter, we introduce some basic principles for presenting summary data, examine some graph types commonly used in biological reports, and de- scribe how to prepare frequency distributions as ta- bles and as graphs. Instructions for using R code to create graphs are given in Applications in R (fol- lowing the chapter 3 exercises). Similar graphs may be made in many other programs. Organizing numerical data into logical and clear tables and graphs is an essential first step in any data analysis. Such displays help researchers view trends in the data, check assumptions about underlying distributions (e.g., normality, section 8.4), and catch mathematical or data entry errors. Thus, a general rule of thumb is to always plot your data. By violating this simple rule, one runs the risk of having to repeat a large suite of analyses when mistakes could have been easily avoided.",STAT
"Figure 3.2A shows that the two different com- ponents of ACT scores appear to be correlated with each other; in other words, students scoring high on the verbal portion of the ACT tend to also score high on the quantitative portion. We wouldn’t want to say that one causes the other; more likely, stu- dents with high scores on one part of the exam have the intellectual abilities and study skills that con- tribute to high scores on the other part. Figure 3.2B shows the dependent relationship of fish metabolic rate (the response variable) on environmental tem- perature in experimental chambers (the explana- tory variable). The strong dependence of metabolic rate on temperature implies a causal relationship in these ectothermic organisms. Notice that the regres- sion line is shown on graph B, but not on graph A, where there is no causal relationship implied and only the broad pattern is of interest. Notice also that for graph B, the response variable is plotted on the vertical axis and the explanatory variable on the horizontal axis. This choice of axes is the conven- tion for such regression data. In contrast, for corre- lation-type graphs it doesn’t matter which variable is plotted on which axis. Similar graphs are em- ployed in chapters 13 and 14, where they provide supporting visual evidence for formal statistical tests (correlation and regression analysis).",STAT
"Most statistical programs have graphing capa- bilities. After you have organized and plotted a cou- ple of histograms by hand, you will appreciate this time-saver! The graphs in figure 3.5 were generated with R, using the raw data from table 3.4. In Appli- cations in R (after the chapter 3 exercises), you will find instructions for creating a histogram with R, in- cluding the coding that generated figure 3.5B. Com- puter programs such as R can be very handy tools for creating histograms, allowing for easy changes in the number of bins for the data. That allows lots of trial and error when creating graphs. Excel can also generate nice histograms if you install the Data Analysis “Add-in” under Options. If we measure a characteristic of a number of individuals in a population, we usually find that the measurement differs among individuals. Gener- ally, there is a certain value around which our mea- surements tend to cluster. Statistics gives us ways to describe this central tendency and to describe the variation of individuals from this point. In this chapter we consider several ways to describe the central tendency and the variation around it.",STAT
"It would be quite unusual indeed to have all of a population available for measurement, perhaps only for the rarest of endangered species. Even if we had the complete population at hand, we might not want to go to all the trouble of measuring so many individuals. Accordingly, we can calculate as- pects of central tendency and variation using a ran- dom sample from the population of interest (section 1.2) and use them to estimate the population’s val- ues. These values in the population as a whole are called parameters,1 and they are usually unknown. In a sample they are called statistics. The central tendency of a sample (or a popula- tion) is the value around which observations tend to cluster. Several statistics can be used to describe central tendency. Which statistic is most appropri- ate depends on the type of variable we have and on the shape of its underlying distribution. For cate- gorical variables, both the proportion and mode are useful measures of central tendency.",STAT
"In the table, examine the values for the mean, median, minimum, and maximum in the cow milk data and confirm that they are the same as what we determined by hand or by inspecting the ordered list (example 4.6). Notice the values reported for the first and third quartiles, which dif- fer slightly from our quick approxi- mation method. Sample size is indicated by the term “length.”4 Consider again the data on fe- male mosquito fish that you ana- lyzed above. The mean for this sample is 34.29 mm (table 4.2). Notice in figure 4.4 that this value is near the center of the distribution. Furthermore, because of the skewed shape of the distribution, the most frequently occurring size class (mode = 29) is smaller than the mean value and the median (33) falls between the mean and the mode. Refer again to figure 4.2; do the relative positions of these estimates follow our expectation for a right-skewed distribution? We have determined a variety of statistics for central tendency and other locations in a sample distribution. We are now ready to consider the vari- ation within a dataset, the stuff that makes statistics necessary and, quite honestly, most interesting!",STAT
"Parameters and statistics are not the same thing. If you calculate a sample standard deviation (s), it is incorrect to label this number as the population standard devia- tion (σ). Mixing up the labeling of statistics and parame- ters is a common mistake made by students of statistics. As we shall see later, the SS is a very important quantity in many statistical procedures. In the above example, the sum of squares is 12 (table 4.3). If the sum of squares is divided by the number of observations minus one (n – 1), we obtain an- other important value, called the variance (s2). Ac- cording to statistical theory, the term n – 1 (called degrees of freedom) is used to correct for a slight bias in estimating the variance (Sokal and Rohlf 2012). If we had a complete census of the popula- tion and calculated the population standard devia- tion, this correction would not be necessary.",STAT
"Review the descriptive statistics shown in table 4.4. These statistics were generated with R. The code for doing descriptive statistics is described in Applications in R, which follows the chapter 4 exer- cises. In it, you can also find a brief description of some unfamiliar terms in the table. The mean and standard deviation are useful de- scriptive statistics for symmetrical, bell-shaped dis- tributions, a property of many familiar variables such as heights, weights, and yields. These statistics are useful in another way when thinking about data. The empirical rule states that about 68% of the data should fall within one standard deviation of the mean, about 95% should fall within two standard deviations, and nearly all of the data should fall within three standard deviations. For example, if the mean and standard deviation of a sample were 50 and 5, respectively, we should expect 95% of the ob- servations to fall between values of 40 and 60 (two standard deviations to either side of the mean). This empirical rule gives us a measuring stick for draw- ing a bell-shaped distribution for any particular data set, as long as the underlying distribution is normal. We explore this topic in more detail in chapter 8.",STAT
"In this chapter, we have explored descriptive statistics for determining central tendency and dis- persion in a sample. Such data analysis is useful, but not terribly interesting since, so far, the results apply only to that particular sample. The broader questions often require us to make inferences about the larger population (section 1.2). When we calcu- late statistics like the sample mean, we are generally interested in knowing something about the popula- tion mean (a parameter, μ). How close can we ex- pect such estimates to be to the true value of the parameter? Basically, we will want to know how to determine the uncertainty of such population esti- mates (chapters 8 and 9). In order to do that, statisti- cians must first know something about probability. We explore that topic in the next chapter. As a general rule for hand-worked problems, show all steps in your calculations and underline your final answer. When running analyses with the computer, save all the relevant output in a format you can easily retrieve.",STAT
"Most events in life are uncertain. Will it rain to- day? Will the batter strike out? Will the phone ring to- night? Probability is the chance that a particular event will occur in an uncertain world. Probabilities range from 0 to 1. When the probability is 0, the event is impossible; and when the probability is 1, the event is certain. The higher the probability, the more certain we can be that the event will occur. Consider a simple thought experiment. Suppose we have a very large population of fish, well mixed and with mutants con- sisting of exactly 10% of the population. If we were to sample one fish from the population and return it, and repeat this process thousands of times, then the probability of obtaining a mutant fish would be equal to the proportion of times that a mutant appeared in the sample. In this case, the probability is 0.10. Proba- bility theory is the formal study of such chance events. Probability theory generates expectations that support informal decisions in such disparate fields as business, sports, and gambling. (Indeed, the for- mal rules of probability were developed in the 1600s by mathematicians employed by gamblers; now they are employed by insurance companies!) Proba- bility theory is used in a number of sciences, includ- ing physics, computer science, artificial intelligence, game theory, and genetics. For instance, genetic counselors can use patterns of inheritance together with outcomes of medical tests to predict the risk of numerous diseases. Probability is also a foundation of statistical inference (hypothesis testing), which we explore extensively later in this book.",STAT
"Note that without the prediction based on the bi- nomial probability distribution, we would not know if the number of predominantly single-sex litters that we observed was too large to be produced by ran- dom combinations of X and Y chromosomes. In other words, we cannot conclude that a result is not due to chance alone unless we know what result is expected if it is due to chance. This concept is the basis for much of statistical hypothesis testing (chapter 6), the subject of much of the remainder of this book. Another discrete probability distribution often useful to biologists is the Poisson distribution. Generally, we use the Poisson distribution to pre- dict probabilities of the occurrences of “rare” events, when such occurrences are known to be in- dependent of one another. We can then use these probabilities to generate an expectation for evaluat- ing if the occurrences of such events are indepen- dent of one another. The event of interest must be rare, which is to say that the number of occurrences in any sampling unit must be small relative to the number of times that it could conceivably occur. In the- ory, there is no upper limit on the possible number of occurrences in a single sampling unit. In reality, there must be limits. For example, when trees are at low density (say one or two per hectare), it’s possi- ble for them to follow a Poisson distribution. How- ever, when the trees are so abundant that they are bumping into one another, the trees cannot be ran- dom in their distribution because they are compet- ing for limited space and resources.",STAT
"Quite often we have multiple measurement variables and would like to quickly assess which variables are associated with which other variables. We might have measures of a number of environ- mental features and would like to know how each feature tends to associate with all other features. Let’s say we had nine such features. Nine items taken two at a time would give 36 possible pair- wise combinations (section 5.3). Doing so many cor- relations requires a lot of calculations. Fortunately, computers are good at this. To illustrate a correla- tion matrix, we’ll use correlation statistics calcu- lated from a survey of 138 lakes (example 13.2). Computer statistical packages typically show correlation results in the form of a matrix, with val- ues of the statistic (r) listed for each possible pair. Such a result is shown in table 13.2. The statistics were generated by R (explained further in Applica- tions in R, following the chapter exercises). In the table, the diagonal shows correlations of each vari- able with itself (values of 1.00). The r-statistics of in- terest fall below the diagonal. For instance, the correlation between area and pH is –0.127. If p-val- ues are not reported automatically (as in our exam- ple), one can easily check table A.7 for the critical value and quickly screen the values of r that equal or exceed that critical value. For the correlation ma- trix shown in table 13.2, the original data has n = 138 lakes in which each of the variables was mea- sured. So, we have 138 – 2 = 136 degrees of freedom.",STAT
"Notice that both egg number and body weight are random variables, in contrast to the data from example 14.1 where temperature was controlled ex- perimentally. Inspection of figure 14.3 indicates that egg production is closely related to body mass in green iguanas and the relationship appears to be increasing linearly. Since regression analyses are normally performed using computer analysis pack- ages, we present the analysis of example 14.2 from R in table 14.6. Examining the ANOVA table from R, we see that the null hypothesis of b = 0 can be rejected since the p-value for the regression is 0.000006, quite small by any standard. Therefore, the statistics confirm that variation in body weight predicts egg production in iguanas. Based on the coefficients reported in table 14.6, the regression equation is: eggs = 1.455 + 32.066 (mass) reflecting the clear positive relationship shown in the figure. R also conveniently calculates the r2 (0.906) and the standard error for the estimate of β (3.444). R also provides an adjusted r2, which we can ignore here. The adjusted r2 involves a much more complicated calculation and this statistic is important only for multiple regression analyses (section 14.10).",STAT
"Statistics are widely used in science (and every- day life) to determine the meaning of raw numeri- cal data. Sometimes we can stop there. For instance, if we study a small population from which we can collect every individual measurement, then we can directly calculate the parameter of interest. Sports enthusiasts may keep track of batting averages for their favorite baseball players (every batting at- tempt is recorded). Fisheries biologists may need to keep track of fungus infections in their hatchery tanks. Draining the tank and counting the number of infected and uninfected fish allows them to de- termine exactly the percent infected. Educators may want to know the average score on an exam for their classes each year, and can simply calculate each mean directly. In each case, since our popula- tion was collected in its entirety, we have directly determined the value for the parameter of interest. On the other hand, if the population is much larger, we must instead take a random sample, cal- culate sample statistics, and then make inferences about parameters of the entire population. (Review concepts of sampling and parameters in sections 1.2 and 4.1, respectively). Consider two examples that illustrate this process. Education is a process of development which includes the three major activities, teaching, training and instruction. Teaching is social as well as a professional activity. It is science as well as art. Modern education is not in a sphere but it has a long and large area of study. Now a days most part of the world population is facing different problems related with the nature and they are studying the solutions to save the nature and global problems, but on the second hand we even today do not try to understand our local problems related to the nature. So for the awareness of the problems of nature and pollution the higher education commission has suggested to add the Environmental Science in the course of different levels. Environmental Science is also well known as Environmental Studies in the Indian Colleges and Universities. Before that it was the part of the science but now a days it is a very common subject and higher education commission has suggested including it as a general paper in all the courses.",ENVS
"Awareness in the field of environmental sciences is becoming a global talk. People worldwide are realizing its importance as they are able to smell a Polluted tomorrow. Careful handling of todays’ environment would only serve as a legacy for tomorrows’ generation. Hence, we need to be judicious in exploiting our resources optimally. To ensure a sustainable development we need to know something about how our environment works. Environment can be defined as the set of conditions that surround an organism or the complex of socio cultural condition that affect an individual. Environmental Science is the systematic, scientific study of the environment in combination with living organisms. Most of the universities have introduced this new content as course of Environmental Science or Environmental Studies/Environmental Science in B.Ed. Course. The present book has been written by including some content of print and non-print media. Now this book is especially for modified syllabus of B.T.C./B.Ed./ M.Ed. of Indian & Foreign Universities/ Training Institute & Education Colleges Recognized by National Council of Teacher Education, New Delhi.",ENVS
"The physical attributes are the answer to this question, which become environment. In fact, the concern of all education is the environment of man. However, man cannot exist or be understood in isolation from the other forms of life and from plant life. Hence, environment refers to the sum total of condition, which surround point in space and time. The scope of the term Environment has been changing and widening by the passage of time. In the primitive age, the environment consisted of only physical aspects of the planted earth' land, air and water as biological communities. As the time passed on man extended his environment through his social, economic and political functions. The answer to this question. It is in nature that physical component of the plant earth, viz land, air, water etc., support and affect life in the biosphere. According to a Goudie, environment is the representative of physical components of the earth where in man is an important factor affecting the environment.",ENVS
"Importance of Environment Studies: The environment studies enlighten us, about the importance of protection and conservation of our indiscriminate release of pollution into the environment. At present a great number of environment issues, have grown in size and complexity day by day, threatening the survival of mankind on earth. We study about these issues besides and effective suggestions in the Environment Studies. Environment studies have become significant for the following reason. Development, in its wake gave birth to Urbanization, Industrial Growth, Transportation Systems, Agriculture and Housing etc. However, it has become phased out in the developed world. The North, to cleanse their own environment has, fact fully, managed to move ‘dirty’ factories of South. When the West developed, it did so perhaps in ignorance of the environmental impact of its activities. Evidently such a path is neither practicable nor desirable, even if developing world follows that. World census reflects that one in every seven persons in this planted lives in India. Evidently with 16 per cent of the world's population and only 2.4 per cent of its land area, there is a heavy pressure on the natural resources including land. Agricultural experts have recognized soils health problems like deficiency of micronutrients and organic matter, soil salinity and damage of soil structure.",ENVS
"Our survival and sustenance depend. Resources withdraw, processing and use of the product have all to by synchronised with the ecological cycles in any plan of development our actions should be planned ecologically for the sustenance of the environment and development. Keeping in view the of goal of planning for environmentally sustainable development India contributed to the United Nations Conference on Environment and Development (UNCED), also referred to as “Earth Summit” held at Rio de Janciro, the Capital of Brazil, 3rd-14th June, 1992. It is essential to make the public aware of the formidable consequences of the Environmental Degradation, if not retorted and reformative measures undertaken, would result in the extinction of life. We are facing various environmental challenges. It is essential to get the country acquainted with these challenges so that their acts may be eco-friendly. Some of these challenges are as under: India has often been described a rich land with poor people. The poverty and environmental degradation have a nexus between them. The vast majority of our people are directly dependent on the nature resources of the country for their basic needs of food, fuel shelter and fodder. About 40% of our people are still below the poverty line. Environment degradation has adversely affected the poor who depend upon the resources of their immediate surroundings. Thus, the challenge of poverty and the challenge environment degradation are two facets of the same challenge. The population growth is essentially a function of poverty. Because, to the very poor, every child is an earner and helper and global concerns have little relevance for him.",ENVS
"Nearly 27 per cent Indians live in urban areas. Urbanisation and industrialisation has given birth to a great number of environmental problem that need urgent attention. Over 30 percent of urban Indians live in slums. Out of India’s 3,245 towns and cities, only 21 have partial or full sewerage and treatment facilities. Hence, coping with rapid urbanization is a major challenge. Proper measures to conserve genetic diversity need to be taken. At present most wild genetic stocks have been disappearing from nature. Wilding including the Asiatic Lion are facing problem of loss of genetic diversity. The protected areas network like sanctuaries, national parks, biosphere reserves are isolating populations. So, they are decreasing changes of one group breeding with another. Remedial steps are to be taken to check decreasing genetic diversity. The people should be roused to orient institutions, attitudes and infrastructures, to suit conditions and needs today. The change has to be brought in keeping in view India’s traditions for resources use managements and education etc. Change should be brought in education, in attitudes, in administrative procedures and in institutions. Because it affects way people view technology resources and development.",ENVS
"Although physical and social environment are common to the individual in a specific situation. Yet every individual has his own psychological environment, in which he lives. Kurt Lewin has used the term ‘life space’ for explaining psychological environment. The Psychological environment enables us to understand the personality of an individual. Both- the person and his goal form psychological environment. If a person is unable to overcome the barriers, he can either get frustrated or completed to change his goal for a new psychological environment. But adopting this mechanism, the individual is helped in his adjustment to the environment. Social Environment includes an individual’s social, economic and political condition wherein he lives. The moral, cultural and emotional forces influence the life and nature of individual behaviour. Society may be classified into two categories as under: Thus, the biotic environment further be divided into floral environment and faunal environment. All the organisms work to form their social groups and organizations at several levels. Thus, the social environment is formed. In this social environment the organisms work to derive matter from the physical environment for their sustenance and development. This process gives birth to economic environment. Man claims to be most skilled and civilized of all the organisms. This is the reason why his social organisation is most systematic. The three aspects of man, e.g. physical, social and economic, function in the biotic environment as under:",ENVS
"In Singrauli complex forests and hillocks have been erased due to construction of high power transmission lines, roads and rail tracks. Establishment of other factories as cement and super thermal power stations around coal mines have resulted into environment degradation. The creation of new settlements for the workmen and rehabilitation of project outsees in the watershed areas may result in the aggravation of the seriousness of advance impacts. In our country a number of big, medium and minor dams are undertaken mainly for three purposes-irrigation, power generation and water supply. The country’s first Prime Minister, Jawharlal Nehru, hailed these dams as the Temples of Modern India. They have increased agricultural production, power generation and reduced dependence in imports. However, on the contrary to the advantages enumerated above, some experts opine that the social, environmental and even economic cost of these dams, far outweighs their benefits. They hold that the most important social consequences of big dams has been the displacement of million of tribals from their homeland and their eventual influx into urban areas, almost as refugees. This is the reason why the scientists, environmentalists, journalists, social activists, lawyers and bureaucrats have now raised their voice against big dams.",ENVS
"In hilly tracts, blasting operations for road construction can cause considerable damage to the environment through the following activities: Ponds have little vertical stratification. In them littoral zone is large than and limnetic zone and profundal zone. In a small pond the limnetic profundal zones are not found. The warm top layer, the epilimnion is heated by the sun and homogenised by the wind and other currents. On the contrary to it, the deep cold layer, the hypolimnion is not heated by sun and not circulated by wind. The basis upon which the layers are maintained is strictly thermal and is caused by the fact that the warmer water is lighter than the colder water. After the formation of a thermocline, no exchange of water occurs between the epilimnion and hypolimnion. Moving water or lotic ecosystems include rivers, streams, and related environments. They are of various sizes ranging from Ganga, Yamuna, Hindon, Kali Nadi, Sutlez, Gomti, etc to the trickle of a small spring. Likewise, there is distinction on the basis of flow. On one hand there are raging torrents and waterfalls and on the other hand, the rivers whose flow is so smooth as to be almost unnoticeable. Every river varies considerably over its length, as it charges from a mountain brook to a large river.",ENVS
"The Ministry of Water Resources constituted in February, 1990 an Environment Monitoring Committee under the Chairmanship of Member, Water Planning, Central Water Commission, with representatives of concerned Ministries to have periodical and effective monitoring of the implementation of environmental conditions laid down by the Ministry of Environment and Forests at the time of clearance of projects. Out of the 82 Irrigation Multi- purpose and Flood Control Projects for which the Ministry of Environmental Monitoring Organization of Central Water Commission has stipulated environmental safeguards and site visits by the committee. The project authorities of the remaining 72 projects have been requested to set-up Project-Level Environment Monitoring Committee and Report the progress to the Environmental Monitoring Committee. The committee has already visited and assessed the situation in respect of Three Projects identified for close monitoring. The Central Soil and Material Research Station, New Delhi, is a premier organization which deals with Geo-mechanics and construction of material problems relevant River Valley Project Construction. It plays an active role in imparting knowledge to Engineers involved in the construction, designs etc. by holding nation level workshops with the help of the United Nations Development Programme Experts.",ENVS
"Protection of the environment and safeguarding of health through the integrated management of water resources and liquid and solid waters. Some 55 pilot Project called mini-mission areas covering various status and union territories and Five Submission were taken up during the seventh plan. A problem village has been defined as one with no source of safe drinking water within a distance of 1.6. or within a depth of 15 metres. One problem villagers face are those where available water has excessive salinity, iron, fluoride or other toxic elements or where diseases like cholera, guinea worm, etc are endemic. After covering problem villagers identified in the Sixth and Seventh Plan, water supply facilities were proposed to be extended to villagers as per liberalized norms i.e. within a distance of 0.5 km. and enhancing present norm of water supply from 40 litres to 70 litres per capita per day and provide one source (tubewell with hand-pumps or stand-post) for a population of 150 against the existing norm of 250-300 persons. Priority was being accorded for coverage of SC/ST habitations and water supply for the economically and socially background areas. States were advised to allocate atleast 25 per cent of ARWSP funds for the Schedule Castes and another ten per cent of Schedule Tribes. At the commencement of the seventh plan, 161, 722 problem villages remained to be covered with safe drinking water facilities. The mission had been successful in covering 1,53,390 problem villages in the Seventh Plan. Remaining 8,332 villages which had spilled over to the Eight Plan were to be covered in the first two years of the Eight Plan i.e. by 1992.",ENVS
"The Narmada Control Authority was set-up in pursuance of the decision of the Narmada Water Disputes Tribunal. It started functioning from December, 1980 and was further strengthened during 1987 and 1990. The authority coordinates and directs Narmada Basin Development Project and takes such measures as are necessary or expedient for protection of environment and also prepares Schemes for the Welfare and Rehabilitation of Oustees and Other Affected Persons. The Tungabhadra Board is incharge of the common portions of the Tungabhadra Project. The Krishna Water Disputes Tribunal had made specific provision in the award for the use of Tungabhadra Waters by Karnataka and Andhra Pradesh. The responsibility for carrying out this specific provision relating to the use of Tungbhadra Water has been entrusted to the Tungabhadra Board by the Tribunal. The board is also regulating the water for irrigation, Hydro-Power Generation and other uses of the Right Bank. India and Pakistan signed the Indus Water Treaty on 19 September, 1960, fixing and delimiting the rights and obligations of the Two Countries with regards to the use of the waters of the Indus River System. It came into force from first April, 1960. A Permanent Indus Commission representing both the Governments have established co-operation arrangements for implementation of the treaty.",ENVS
"The Indo-Nepal, sub-commission on water resources was set-up in August 1988, to deal with all aspects of Indo-Nepal Cooperation in the multiple uses of Water Resources for mutual benefit. In additional to the matter already under discussion at secretary-level and other meetings, the sub-commission shall identify new programes/Project for water resources development for cooperation between India and Nepal in specific sectors viz. irrigation, water-logging and drainage, hydro-electric power generation. Inland navigation, collection of hydrological data, measures to prevent and reduce losses due to floods, flood forecasting and flood warning, environment safeguard measures and transfer of technology suited to the requirement of both the countries. The Farkka Barrage Project is designed to subserve the need for preservation and maintenance of the Calcutta Port by improving the regime and navigability of the Bhagirathi- Hooghly River System. The Bhagirathi, the feeder canal and the navigation lock at the Farakka Barrage form part of the Haldi-Allahabad Inland Waterway for which an act has been passed.",ENVS
"The above-mentioned problems created by the dams have led to the opposition in various corners of the country. The experts hold it from time to time that the social, environmental and even economic cost of these dams, however, far outweighs their benefits. The most important social consequence of big dams has been displacement of millions of tribals from their homeland and their eventual influx into urban areas, almost as refugees. This is the reason why Scientists, environmentalists, journalists, social activities, lawyers and bureaucrats have taken up the cry against big dams. The ever-increasing opposition from scientists and environmentalists has forced the Govt. of review a number of proposed dams in the light of their impact on local tribals flora and fauna. Results are also seen. The Govt. had to scrap the Silent Valley Project in Kerala. Likewise, Koel and Karo Project in Bihar was also abandoned due to opposition from local people as it would have displaced several thousands for Santhal tribals in the area.",ENVS
"Though these have been given environment clearance, pressure is being exercised on the Govt. to drop these projects. A reader of newpapers could go through the headlines as “Losses exceed Tehri Dam benefits,” “Govt. forced to rivew dam projects”. “Big dams spell doom”, “How green was my valley”, “The displacement factor” etc. A brief description of these dams is as under: Besides, additional displacement is likely to be caused during social and environmental rehabilitation work undertaken to repair the dislocation and damages caused by the project. Likewise, compensatory afforestation and setting of wildlife sanctuary will displace other villagers in the area. It is officially admitted that nearly 43,000 ha of land will be needed for rehabilitation of SS outees. It claims to be the world’s largest river valley project. The 30 big dams and over 3,000 medium dams are envisaged. It is estimated that it would displace over one million people, mostly tribals, submerge 56,000 ha of fertile agriculture land. Total forest are of nearly 60,000 ha will be destroyed. As a result, nearly 25 species of birds will be deprived of their habitats.",ENVS
"GSI with its headquarters at Calcutta functions six regions, three specialised wings and a training institute. Each year GSI takes up about 1100 investigations in geological mapping, mineral assessment, geotechnical and environment studies, air borne geo-physical surveys as well as geological and geophysical surveys in bordering seas. Output of these efforts is in the form of scientific and technical reports, professional papers, maps and inventories and various types of publications such as memoirs, records, bulletins, Indian minerals and Palentologica Indica. In addition to production of primary metals—zinc and lead, the company produces a number of by-products such as cadmium, silver, sulphuric acid, phosphoric acid, fertilisers, zinc sulphate and copper sulphate. Based on lead-zinc deposits of Rampura-Agucha in Bhillwara District, a new open pit mine at Rampura-Agucha and a new smelter at Chanderiya in Chittogarh District is under construction. Total cost of the project is estimated at Rs. 684 crore. The smelte will have a capacity of 70,000 tonnes zinc and 35,000 tonnes Lead per annum. It will be operational during 1991-92.",ENVS
"Production of quality seed is a vital link in the spread of new varities and, hence, this programme was given a very high-priority by executing National Seed Project. Phase III of this project has been taken up for accelerating the development of High Quality Breeder Seed. Conservation of Valuable Plant Genetic Resources is vital for the success of Crop Improvement Programme. National Bureau of Plant Genetic Resource established in 1976 is acting as a nodal Institute for the important activities related to collection, conservation and exchange of Germ plasm. Lately over 2400 Germ-plasm accessions were added to over one lakh accessions already conserved in the Gene Bank for long-term conservation. To further strengthen the research efforts on conservation has been launched at NBPGR, New Delhi. Sustainability and environmental quality have been the major considerations for Developing Crop Protection Programmes. Major thrust was given in promotion of Integrated Pest Management (IPM) concept in all major crops. This included biology control of crops pests and diseases; mass multiplication and development of appropriate systems for dissemination of biological gents and use of novel methods such as insect growth regulators, Pheromones, Kairmones, etc.",ENVS
"Pesticides widely distributed by natural means but they tend to retain much of their biocidal activity for fairly long periods. On account of the use of different kinds of poisonous agriculture chemicals the whole biosphere is being increasingly poisoned and polluted. Many of these chemicals and pesticides are known to persist for long periods in the environment. Their concentration builds up geometrically as they are transferred to different stages of the food web. Serious cases of fish mortality have occurred following the leaching of poisonous biocides from agricultural fields to nearby rivers or streams after rainfall. Great concern was shown on a case of large-scale fish kill in the lower Mississippi river in U.S.A. wherein five million fish died. Careful investigation indicated that the fish had died due to dumping of Endrin- rich agriculture wastes and runoff into a tributary of the Mississippi River the Memphis. Residues of various weedicides and insecticides often accumulate in agricultural soils rapidly. Insecticides are designed to kill insects. As such they may not be toxic to plants. On the contrary to it, some herbicides differ from insecticides in killing both desirable species as well as the intended target. They may adversely affect such soils microbes as nitroes fixing blue-green algae and bacteria. This, in turn, may impair the growth and production of higher plants.",ENVS
"Most pesticides tend to accentuate the problems of both production and pollution instead of containing them. The consequence of pesticides is almost invariably adverse and harmful. In the Ninteenth century, the ladybird beetle was brought from Australia to California to control a scale insect pest of oranges. It is reported that the beetle successfully kept the pest under check for more than five decades until about 1946 when DDT began to be used in the citrus orchards. The beetle was susceptible to DDT and hence its population declined. However, a subsequent withdrawal of DDT again restored the natural balance of biological control within a few years. In fact, DDT is one of the most effective pesticides known. This is the reason why it was banned in the USA in 1972. its remarked insecticidal properties were first discovered in 1939. it became a ubiquitous contaminant of fish, penguins, birds and human being. Hence a popular public movement started in the USA that asked the Government to protect the public from the general toxification of the environment by DDT and persistent poisons.",ENVS
"Pesticides have harmful effects on insects, earthworms, invertebrates, protozoa, and microbes found in soils, especially the decomposers. It is reported that human pesticide poisonings, reeducation in insects and mites, and honeybee poisonings account for about 70 per cent of the calculated socio-environmental costs for pesticides in the USA. The use of pesticides kills natural enemies and creates such problems as the development of secondary pests eg. Red spider mites. Resurgence of primary pests can also occur. To illustrate, caterpillars of the small cabbage white butterfly in Brussels sprouts reappeared after DDT has killed their natural enemies. Resistance to pesticides is a cause for serious concern. Other hazards include those to the operator or worker who sprays pesticide, those to the consumer of the crop and those to wildlife. Some species of Eagles and top carnivors are known to be eliminated by DDT because contaminated adults failed to lay viable eggs. Populations of peregrine falcons and some pelicans have disappeard from some areas from some areas where excessive use of DDT interfered with the bird’s ability to transport calcium to growing eggs, leading to marked thinning of the eggs shells. Such weak eggs fail to reach the hatching stage. It is discovered that in ringdoves, DDT greatly reduced the activity of carbonic anhydrase. This enzyme is critical in providing calcium for eggshell growth. When the pesticide inhibits this enzyme, eggshell grows thinner. Artificial introduction of pesticides in the environment upsets natural biological controls. This is the reason why new pests are created in this way because their natural predators, which previously checked their populations, are eliminated. In this way mites have become a pest as a consequence of the emergence of the pesticide industry. Indiscriminate and excessive use of DDT killed some insect predictors of these mites, enabling the mites to multiply to pest status.",ENVS
